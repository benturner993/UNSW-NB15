{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, matthews_corrcoef, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "X = pd.read_csv('X.csv')\n",
    "y = pd.read_csv('y.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "# consistency in naming conventions\n",
    "test_df=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training / validation \n",
    "training_df, validation_df, response_training_df, response_validation_df = train_test_split(X, y, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Accuracy Score\n",
    "y_train=response_training_df['label']\n",
    "lst = [0] * len(y_train)\n",
    "print(f'Null Accuracy Score: {accuracy_score(y_train, pd.Series(lst))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join datasets\n",
    "exploration_df=X.merge(y, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rowcount=X.shape[0]\n",
    "cols=[]\n",
    "missing_per=[]\n",
    "\n",
    "for i in list(X.columns):\n",
    "    \n",
    "    # how many are missing (consistent with cell above)\n",
    "    val=(X.loc[(X[i].isnull())].shape[0])/total_rowcount\n",
    "    cols.append(i)\n",
    "    missing_per.append(val)\n",
    "#     print(i, val)\n",
    "    \n",
    "d = {'column_name':cols,'percent_missing':missing_per}\n",
    "missing_df=pd.DataFrame(d)\n",
    "\n",
    "missing_df=missing_df.loc[missing_df['percent_missing']>0]\n",
    "missing_df.sort_values(by='percent_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for dupes\n",
    "print('There are ', X.shape[0], ' rows in the dataset')\n",
    "print('There are ', X.drop_duplicates().shape[0], ' rows after dropping duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exploration_df.shape)\n",
    "\n",
    "for i in list(set(exploration_df['attack_cat'])):\n",
    "    \n",
    "    print('*'*8)\n",
    "    print(i)\n",
    "    trial=exploration_df.loc[exploration_df['attack_cat']==i]\n",
    "    # Are there any constant columns?\n",
    "#     print('All columns:')\n",
    "#     print(trial.columns[trial.nunique() >=0].to_list())\n",
    "\n",
    "    print('\\nColumns with 1 unique level')\n",
    "    print(trial.columns[trial.nunique() ==1].to_list())\n",
    "\n",
    "#     print('\\nColumns with g1 unique levels')\n",
    "#     print(trial.columns[trial.nunique() >1].to_list())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning using sbytes, rate, dbytes, dur, proto, smean\n",
    "training_df['service_rate']=training_df.groupby('service')['rate'].transform('mean')/training_df['rate']\n",
    "training_df['state_rate']=training_df.groupby('state')['rate'].transform('mean')/training_df['rate']\n",
    "training_df['proto_rate']=training_df.groupby('proto')['rate'].transform('mean')/training_df['rate']\n",
    "validation_df['service_rate']=validation_df.groupby('service')['rate'].transform('mean')/validation_df['rate']\n",
    "validation_df['state_rate']=validation_df.groupby('state')['rate'].transform('mean')/validation_df['rate']\n",
    "validation_df['proto_rate']=validation_df.groupby('proto')['rate'].transform('mean')/validation_df['rate']\n",
    "test_df['service_rate']=test_df.groupby('service')['rate'].transform('mean')/test_df['rate']\n",
    "test_df['state_rate']=test_df.groupby('state')['rate'].transform('mean')/test_df['rate']\n",
    "test_df['proto_rate']=test_df.groupby('proto')['rate'].transform('mean')/test_df['rate']\n",
    "\n",
    "training_df['service_sbytes']=training_df.groupby('service')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "training_df['state_sbytes']=training_df.groupby('state')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "training_df['proto_sbytes']=training_df.groupby('proto')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "validation_df['service_sbytes']=validation_df.groupby('service')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "validation_df['state_sbytes']=validation_df.groupby('state')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "validation_df['proto_sbytes']=validation_df.groupby('proto')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "test_df['service_sbytes']=test_df.groupby('service')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "test_df['state_sbytes']=test_df.groupby('state')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "test_df['proto_sbytes']=test_df.groupby('proto')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "\n",
    "training_df['service_dpkts']=training_df.groupby('service')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "training_df['state_dpkts']=training_df.groupby('state')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "training_df['proto_dpkts']=training_df.groupby('proto')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "validation_df['service_dpkts']=validation_df.groupby('service')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "validation_df['state_dpkts']=validation_df.groupby('state')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "validation_df['proto_dpkts']=validation_df.groupby('proto')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "test_df['service_dpkts']=test_df.groupby('service')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "test_df['state_dpkts']=test_df.groupby('state')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "test_df['proto_dpkts']=test_df.groupby('proto')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "\n",
    "training_df['service_dur']=training_df.groupby('service')['dur'].transform('mean')/training_df['dur']\n",
    "training_df['state_dur']=training_df.groupby('state')['dur'].transform('mean')/training_df['dur']\n",
    "training_df['proto_dur']=training_df.groupby('proto')['dur'].transform('mean')/training_df['dur']\n",
    "validation_df['service_dur']=validation_df.groupby('service')['dur'].transform('mean')/validation_df['dur']\n",
    "validation_df['state_dur']=validation_df.groupby('state')['dur'].transform('mean')/validation_df['dur']\n",
    "validation_df['proto_dur']=validation_df.groupby('proto')['dur'].transform('mean')/validation_df['dur']\n",
    "test_df['service_dur']=test_df.groupby('service')['dur'].transform('mean')/test_df['dur']\n",
    "test_df['state_dur']=test_df.groupby('state')['dur'].transform('mean')/test_df['dur']\n",
    "test_df['proto_dur']=test_df.groupby('proto')['dur'].transform('mean')/test_df['dur']\n",
    "                                                                               \n",
    "training_df['service_smean']=training_df.groupby('service')['smean'].transform('mean')/training_df['smean']\n",
    "training_df['state_smean']=training_df.groupby('state')['smean'].transform('mean')/training_df['smean']\n",
    "training_df['proto_smean']=training_df.groupby('proto')['smean'].transform('mean')/training_df['smean']\n",
    "validation_df['service_smean']=validation_df.groupby('service')['smean'].transform('mean')/validation_df['smean']\n",
    "validation_df['state_smean']=validation_df.groupby('state')['smean'].transform('mean')/validation_df['smean']\n",
    "validation_df['proto_smean']=validation_df.groupby('proto')['smean'].transform('mean')/validation_df['smean']\n",
    "test_df['service_smean']=test_df.groupby('service')['smean'].transform('mean')/test_df['smean']\n",
    "test_df['state_smean']=test_df.groupby('state')['smean'].transform('mean')/test_df['smean']\n",
    "test_df['proto_smean']=test_df.groupby('proto')['smean'].transform('mean')/test_df['smean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature imputation with mode value\n",
    "for i in list(X.columns):\n",
    "    \n",
    "    print(i)\n",
    "    training_df[i]=training_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('training_df missing %:', sum(training_df[i].isnull())/training_df[i].shape[0])\n",
    "    validation_df[i]=validation_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('validation_df missing %:', sum(validation_df[i].isnull())/validation_df[i].shape[0])\n",
    "    test_df[i]=test_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('test_df missing %:', sum(test_df[i].isnull())/test_df[i].shape[0], '\\n')\n",
    "    \n",
    "#     training_df[training_df == -inf] = 0\n",
    "#     validation_df[validation_df == -inf] = 0\n",
    "#     test_df[test_df == -inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_binning(training_df, validation_df, test_df, categoric_column, threshold):\n",
    "\n",
    "    # return the cumulative sum of values based on size\n",
    "    cumulative_values=training_df.groupby(categoric_column)[categoric_column].size().sort_values(ascending=False)\\\n",
    "    .div(len(training_df)).cumsum().le(threshold)\n",
    "\n",
    "    # filter threshold\n",
    "    threshold_columns=cumulative_values[cumulative_values].index.values\n",
    "\n",
    "    # apply to dataframe\n",
    "    training_df[categoric_column+'_B']=np.where(training_df[categoric_column].isin(threshold_columns), training_df[categoric_column], 'Other')\n",
    "    validation_df[categoric_column+'_B']=np.where(validation_df[categoric_column].isin(threshold_columns), validation_df[categoric_column], 'Other')\n",
    "    test_df[categoric_column+'_B']=np.where(test_df[categoric_column].isin(threshold_columns), test_df[categoric_column], 'Other')\n",
    "    \n",
    "    # visualise value_counts()\n",
    "    print(training_df[categoric_column+'_B'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_binning(training_df, validation_df, test_df, 'proto', 0.925)\n",
    "categoric_binning(training_df, validation_df, test_df, 'state', 0.99)\n",
    "categoric_binning(training_df, validation_df, test_df, 'service', 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_features(df):\n",
    "\n",
    "    df['login_ports']=df['is_ftp_login'].astype(str)+df['is_sm_ips_ports'].astype(str)\n",
    "    df['login_flow']=df['is_ftp_login'].astype(str)+df['ct_ftp_cmd'].astype(str)\n",
    "    df['login_ports_flow']=df['is_ftp_login'].astype(str)+df['ct_ftp_cmd'].astype(str)+df['is_sm_ips_ports'].astype(str)\n",
    "\n",
    "concat_features(training_df)\n",
    "concat_features(validation_df)\n",
    "concat_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for sttl\n",
    "bins = [0, 25, 50, 100, 200, 1000]\n",
    "labels = [1, 32, 64, 128, 256]\n",
    "\n",
    "training_df['sttl_B'] = pd.cut(training_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)\n",
    "validation_df['sttl_B'] = pd.cut(validation_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)\n",
    "test_df['sttl_B'] = pd.cut(test_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trans Depth Feature\n",
    "training_df['trans_depth_B']=np.where(training_df['trans_depth']>0,1,0)\n",
    "validation_df['trans_depth_B']=np.where(validation_df['trans_depth']>0,1,0)\n",
    "test_df['trans_depth_B']=np.where(test_df['trans_depth']>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total packages features\n",
    "training_df['total_packets_FE']=training_df['dur']*training_df['rate']\n",
    "validation_df['total_packets_FE']=validation_df['dur']*validation_df['rate']\n",
    "test_df['total_packets_FE']=test_df['dur']*test_df['rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoder(training_df, validation_df, test_df, col, response):\n",
    "\n",
    "    \"\"\" function to mean encode categorical features \n",
    "        any missing values are imputed with mode \"\"\"\n",
    "\n",
    "    # Create dictionary\n",
    "    mean_encoding=training_df.groupby(col)[response].mean().to_dict()\n",
    "\n",
    "    # Apply to train\n",
    "    training_df[col+'_ME']=training_df[col].replace(mean_encoding)\n",
    "    training_df[col+'_ME']=training_df[col+'_ME'].fillna(training_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to valid\n",
    "    validation_df[col+'_ME']=validation_df[col].replace(mean_encoding)\n",
    "    validation_df[col+'_ME']=validation_df[col+'_ME'].fillna(validation_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to test\n",
    "    test_df[col+'_ME']=test_df[col].replace(mean_encoding)\n",
    "    test_df[col+'_ME']=test_df[col+'_ME'].fillna(test_df[col+'_ME'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding and Mean encoding\n",
    "num_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "fe_categorical_columns = list(training_df.select_dtypes(exclude=num_dtypes).columns)\n",
    "unwanted = ['proto', 'state', 'service']\n",
    "fe_categorical_columns = [ele for ele in fe_categorical_columns if ele not in unwanted]\n",
    "\n",
    "training_df=training_df.merge(response_training_df, on='id', how='left')\n",
    "\n",
    "for i in fe_categorical_columns:\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    mean_encoder(training_df, validation_df, test_df, i, 'label')\n",
    "    \n",
    "    # Get one hot encoding of columns B\n",
    "    one_hot_train = pd.get_dummies(training_df[i], prefix=i)\n",
    "    one_hot_valid = pd.get_dummies(validation_df[i], prefix=i)\n",
    "    one_hot_test = pd.get_dummies(test_df[i], prefix=i)\n",
    "    \n",
    "    # Drop column B as it is now encoded\n",
    "    training_df = training_df.drop(i,axis = 1)\n",
    "    validation_df = validation_df.drop(i,axis = 1)\n",
    "    test_df = test_df.drop(i,axis = 1)\n",
    "    \n",
    "    # Join the encoded df\n",
    "    training_df = training_df.join(one_hot_train)\n",
    "    validation_df = validation_df.join(one_hot_valid)\n",
    "    test_df = test_df.join(one_hot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "def outlier_capping(training_df, validation_df, test_df, variable, multiplier):\n",
    "\n",
    "    ''' cap and collar the response variable '''\n",
    "\n",
    "    q1 = np.percentile(training_df[variable],25)\n",
    "    q3 = np.percentile(training_df[variable],75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (iqr * multiplier)\n",
    "    upper = q3 + (iqr * multiplier)\n",
    "\n",
    "    # adjust on train\n",
    "    training_df[variable] = np.where(training_df[variable]<=lower, lower, training_df[variable])\n",
    "    training_df[variable] = np.where(training_df[variable]>=upper, upper, training_df[variable])\n",
    "\n",
    "    # adjust on train\n",
    "    validation_df[variable] = np.where(validation_df[variable]<=lower, lower, validation_df[variable])\n",
    "    validation_df[variable] = np.where(validation_df[variable]>=upper, upper, validation_df[variable])\n",
    "\n",
    "    # adjust on train\n",
    "    test_df[variable] = np.where(test_df[variable]<=lower, lower, test_df[variable])\n",
    "    test_df[variable] = np.where(test_df[variable]>=upper, upper, test_df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap all numeric values which arent integers\n",
    "s=(training_df.dtypes==float)\n",
    "\n",
    "for i in s[s].keys():\n",
    "    \n",
    "    print(i)\n",
    "    outlier_capping(training_df, validation_df, test_df, i, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all columns\n",
    "training_columns=list(training_df.columns)\n",
    "validation_columns=list(validation_df.columns)\n",
    "test_columns=list(test_df.columns)\n",
    "\n",
    "# intersection of all common items in train, valid, test\n",
    "modelling_cols=list(set(training_columns).intersection(validation_columns, test_columns))\n",
    "\n",
    "unwanted = ['proto', 'state', 'service', 'id']\n",
    "modelling_cols = [ele for ele in modelling_cols if ele not in unwanted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response variables\n",
    "for i in list(set(response_training_df['attack_cat'])):\n",
    "    \n",
    "    # create response cols\n",
    "    response_training_df[i]=np.where(response_training_df['attack_cat']==i,1,0)\n",
    "    response_validation_df[i]=np.where(response_validation_df['attack_cat']==i,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_algo(algo, X_train, Y_train, X_valid, Y_valid, cv):\n",
    "    \n",
    "    # fit the model\n",
    "    model = algo.fit(X_train, Y_train)\n",
    "    \n",
    "    # check its score\n",
    "    acc = round(model.score(X_train, Y_train) *100, 2)\n",
    "    f1_train = f1_score(Y_train, model.predict(X_train))\n",
    "    f1_test = f1_score(Y_valid, model.predict(X_valid))\n",
    "    print('Training F1 Score:', f1_train)\n",
    "    print('Test F1 Score:', f1_test)    \n",
    "    \n",
    "    y_pred = model_selection.cross_val_predict(algo, X_train, Y_train, cv=cv, n_jobs = -1)\n",
    "    y_pred_test = model_selection.cross_val_predict(algo, X_valid, Y_valid, cv=cv, n_jobs = -1)\n",
    "    f1_train_cv = f1_score(Y_train, y_pred)\n",
    "    f1_test_cv = f1_score(Y_valid, y_pred_test)\n",
    "    print('Cross Validated Training F1 Score:', f1_train_cv)\n",
    "    print('Cross Validated Test F1 Score:', f1_test_cv) \n",
    "    \n",
    "    return model\n",
    "\n",
    "def z_score(df, col):\n",
    "    \n",
    "    return (df[col]-df[col].mean())/df[col].std()\n",
    "\n",
    "def min_max(df, col):\n",
    "    \n",
    "    return (df[col]-df[col].min())/(df[col].max()-df[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i='Normal'\n",
    "\n",
    "print('model #9:', i)\n",
    "\n",
    "# define modelling cols and response variable for Exploits\n",
    "X_train=training_df[modelling_cols]\n",
    "Y_train=response_training_df[i]\n",
    "X_valid=validation_df[modelling_cols]\n",
    "Y_valid=response_validation_df[i]\n",
    "\n",
    "# RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 125, criterion = 'entropy')\n",
    "rf_Normal = fit_algo(clf, X_train, Y_train, X_valid, Y_valid, 3)\n",
    "\n",
    "# Score on test_df\n",
    "training_df[i]=rf_Normal.predict_proba(training_df[modelling_cols])[:,0]\n",
    "training_df[i+'_z']=z_score(training_df, i)\n",
    "\n",
    "validation_df[i]=rf_Normal.predict_proba(validation_df[modelling_cols])[:,0]\n",
    "validation_df[i+'_z']=z_score(validation_df, i)\n",
    "\n",
    "test_df[i]=rf_Normal.predict_proba(test_df[modelling_cols])[:,0]\n",
    "test_df[i+'_z']=z_score(test_df, i)\n",
    "\n",
    "training_df[i+'_m']=min_max(training_df, i)\n",
    "validation_df[i+'_m']=min_max(validation_df, i)\n",
    "test_df[i+'_m']=min_max(test_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['classification_prediction']=training_df['Normal']\n",
    "validation_df['classification_prediction']=validation_df['Normal']\n",
    "test_df['classification_prediction']=test_df['Normal']\n",
    "\n",
    "test_df['classification_prediction']=np.where(test_df['classification_prediction']>test_df['classification_prediction'].mean(),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-Class modelling\n",
    "\"\"\"\n",
    "\n",
    "# feature selection\n",
    "mc_X_train=training_df[modelling_cols]\n",
    "mc_Y_train=response_training_df['attack_cat']\n",
    "mc_X_valid=validation_df[modelling_cols]\n",
    "mc_Y_valid=response_validation_df['attack_cat']\n",
    "\n",
    "\"\"\"# transform based on submission criteria\n",
    "response_training_df['attack_cat']=response_training_df['attack_cat'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "[0,1,2,3,4,5,6,7,8,9])response_validation_df['attack_cat']=response_validation_df['attack_cat'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "[0,1,2,3,4,5,6,7,8,9])\"\"\"\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix# Create the SVM\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 140, criterion = 'entropy', class_weight='balanced')# Make it an OvR classifier\n",
    "ovr_classifier = OneVsRestClassifier(rfc)# Fit the data to the OvR classifier\n",
    "ovr_classifier = ovr_classifier.fit(mc_X_train, mc_Y_train)# Evaluate by means of a confusion matrix\n",
    "matrix = plot_confusion_matrix(ovr_classifier, mc_X_valid, mc_Y_valid,\n",
    "cmap=plt.cm.Blues,\n",
    "normalize='true')\n",
    "plt.title('Confusion matrix for OvR classifier')\n",
    "plt.show(matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['classification_prediction']=np.where(test_df['classification_prediction']>test_df['classification_prediction'].mean(),1,0)\n",
    "test_df['output_classes']=ovr_classifier.predict(test_df[modelling_cols])\n",
    "test_df['attack_cat_prediction']=test_df['output_classes'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "                                     [0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score xgb_model on test\n",
    "y_test_id = test_df['id']\n",
    "y_test_label_pred = test_df['classification_prediction']\n",
    "y_test_attack_cat_pred = test_df['attack_cat_prediction']\n",
    "\n",
    "# export for submission\n",
    "y_pred = pd.DataFrame(data = {'id': y_test_id, 'label' : y_test_label_pred, 'attack_cat' : y_test_attack_cat_pred})\n",
    "y_pred.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
