{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, matthews_corrcoef, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "X = pd.read_csv('X.csv')\n",
    "y = pd.read_csv('y.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "# consistency in naming conventions\n",
    "test_df=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training / validation \n",
    "training_df, validation_df, response_training_df, response_validation_df = train_test_split(X, y, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Accuracy Score\n",
    "y_train=response_training_df['label']\n",
    "lst = [0] * len(y_train)\n",
    "print(f'Null Accuracy Score: {accuracy_score(y_train, pd.Series(lst))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join datasets\n",
    "exploration_df=X.merge(y, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rowcount=X.shape[0]\n",
    "cols=[]\n",
    "missing_per=[]\n",
    "\n",
    "for i in list(X.columns):\n",
    "    \n",
    "    # how many are missing (consistent with cell above)\n",
    "    val=(X.loc[(X[i].isnull())].shape[0])/total_rowcount\n",
    "    cols.append(i)\n",
    "    missing_per.append(val)\n",
    "#     print(i, val)\n",
    "    \n",
    "d = {'column_name':cols,'percent_missing':missing_per}\n",
    "missing_df=pd.DataFrame(d)\n",
    "\n",
    "missing_df=missing_df.loc[missing_df['percent_missing']>0]\n",
    "missing_df.sort_values(by='percent_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature imputation with mode value\n",
    "for i in list(X.columns):\n",
    "    \n",
    "    print(i)\n",
    "    training_df[i]=training_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('training_df missing %:', sum(training_df[i].isnull())/training_df[i].shape[0])\n",
    "    validation_df[i]=validation_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('validation_df missing %:', sum(validation_df[i].isnull())/validation_df[i].shape[0])\n",
    "    test_df[i]=test_df[i].fillna(training_df[i].mode()[0])\n",
    "    print('test_df missing %:', sum(test_df[i].isnull())/test_df[i].shape[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for dupes\n",
    "print('There are ', X.shape[0], ' rows in the dataset')\n",
    "print('There are ', X.drop_duplicates().shape[0], ' rows after dropping duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exploration_df.shape)\n",
    "\n",
    "for i in list(set(exploration_df['attack_cat'])):\n",
    "    \n",
    "    print('*'*8)\n",
    "    print(i)\n",
    "    trial=exploration_df.loc[exploration_df['attack_cat']==i]\n",
    "    # Are there any constant columns?\n",
    "#     print('All columns:')\n",
    "#     print(trial.columns[trial.nunique() >=0].to_list())\n",
    "\n",
    "    print('\\nColumns with 1 unique level')\n",
    "    print(trial.columns[trial.nunique() ==1].to_list())\n",
    "\n",
    "#     print('\\nColumns with g1 unique levels')\n",
    "#     print(trial.columns[trial.nunique() >1].to_list())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning using sbytes, rate, dbytes, dur, proto, smean\n",
    "training_df['service_rate']=training_df.groupby('service')['rate'].transform('mean')/training_df['rate']\n",
    "training_df['state_rate']=training_df.groupby('state')['rate'].transform('mean')/training_df['rate']\n",
    "training_df['proto_rate']=training_df.groupby('proto')['rate'].transform('mean')/training_df['rate']\n",
    "validation_df['service_rate']=validation_df.groupby('service')['rate'].transform('mean')/validation_df['rate']\n",
    "validation_df['state_rate']=validation_df.groupby('state')['rate'].transform('mean')/validation_df['rate']\n",
    "validation_df['proto_rate']=validation_df.groupby('proto')['rate'].transform('mean')/validation_df['rate']\n",
    "test_df['service_rate']=test_df.groupby('service')['rate'].transform('mean')/test_df['rate']\n",
    "test_df['state_rate']=test_df.groupby('state')['rate'].transform('mean')/test_df['rate']\n",
    "test_df['proto_rate']=test_df.groupby('proto')['rate'].transform('mean')/test_df['rate']\n",
    "\n",
    "training_df['service_sbytes']=training_df.groupby('service')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "training_df['state_sbytes']=training_df.groupby('state')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "training_df['proto_sbytes']=training_df.groupby('proto')['sbytes'].transform('mean')/training_df['sbytes']\n",
    "validation_df['service_sbytes']=validation_df.groupby('service')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "validation_df['state_sbytes']=validation_df.groupby('state')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "validation_df['proto_sbytes']=validation_df.groupby('proto')['sbytes'].transform('mean')/validation_df['sbytes']\n",
    "test_df['service_sbytes']=test_df.groupby('service')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "test_df['state_sbytes']=test_df.groupby('state')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "test_df['proto_sbytes']=test_df.groupby('proto')['sbytes'].transform('mean')/test_df['sbytes']\n",
    "\n",
    "training_df['service_dpkts']=training_df.groupby('service')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "training_df['state_dpkts']=training_df.groupby('state')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "training_df['proto_dpkts']=training_df.groupby('proto')['dpkts'].transform('mean')/training_df['dpkts']\n",
    "validation_df['service_dpkts']=validation_df.groupby('service')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "validation_df['state_dpkts']=validation_df.groupby('state')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "validation_df['proto_dpkts']=validation_df.groupby('proto')['dpkts'].transform('mean')/validation_df['dpkts']\n",
    "test_df['service_dpkts']=test_df.groupby('service')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "test_df['state_dpkts']=test_df.groupby('state')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "test_df['proto_dpkts']=test_df.groupby('proto')['dpkts'].transform('mean')/test_df['dpkts']\n",
    "\n",
    "training_df['service_dur']=training_df.groupby('service')['dur'].transform('mean')/training_df['dur']\n",
    "training_df['state_dur']=training_df.groupby('state')['dur'].transform('mean')/training_df['dur']\n",
    "training_df['proto_dur']=training_df.groupby('proto')['dur'].transform('mean')/training_df['dur']\n",
    "validation_df['service_dur']=validation_df.groupby('service')['dur'].transform('mean')/validation_df['dur']\n",
    "validation_df['state_dur']=validation_df.groupby('state')['dur'].transform('mean')/validation_df['dur']\n",
    "validation_df['proto_dur']=validation_df.groupby('proto')['dur'].transform('mean')/validation_df['dur']\n",
    "test_df['service_dur']=test_df.groupby('service')['dur'].transform('mean')/test_df['dur']\n",
    "test_df['state_dur']=test_df.groupby('state')['dur'].transform('mean')/test_df['dur']\n",
    "test_df['proto_dur']=test_df.groupby('proto')['dur'].transform('mean')/test_df['dur']\n",
    "                                                                               \n",
    "training_df['service_smean']=training_df.groupby('service')['smean'].transform('mean')/training_df['smean']\n",
    "training_df['state_smean']=training_df.groupby('state')['smean'].transform('mean')/training_df['smean']\n",
    "training_df['proto_smean']=training_df.groupby('proto')['smean'].transform('mean')/training_df['smean']\n",
    "validation_df['service_smean']=validation_df.groupby('service')['smean'].transform('mean')/validation_df['smean']\n",
    "validation_df['state_smean']=validation_df.groupby('state')['smean'].transform('mean')/validation_df['smean']\n",
    "validation_df['proto_smean']=validation_df.groupby('proto')['smean'].transform('mean')/validation_df['smean']\n",
    "test_df['service_smean']=test_df.groupby('service')['smean'].transform('mean')/test_df['smean']\n",
    "test_df['state_smean']=test_df.groupby('state')['smean'].transform('mean')/test_df['smean']\n",
    "test_df['proto_smean']=test_df.groupby('proto')['smean'].transform('mean')/test_df['smean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_binning(training_df, validation_df, test_df, categoric_column, threshold):\n",
    "\n",
    "    # return the cumulative sum of values based on size\n",
    "    cumulative_values=training_df.groupby(categoric_column)[categoric_column].size().sort_values(ascending=False)\\\n",
    "    .div(len(training_df)).cumsum().le(threshold)\n",
    "\n",
    "    # filter threshold\n",
    "    threshold_columns=cumulative_values[cumulative_values].index.values\n",
    "\n",
    "    # apply to dataframe\n",
    "    training_df[categoric_column+'_B']=np.where(training_df[categoric_column].isin(threshold_columns), training_df[categoric_column], 'Other')\n",
    "    validation_df[categoric_column+'_B']=np.where(validation_df[categoric_column].isin(threshold_columns), validation_df[categoric_column], 'Other')\n",
    "    test_df[categoric_column+'_B']=np.where(test_df[categoric_column].isin(threshold_columns), test_df[categoric_column], 'Other')\n",
    "    \n",
    "    # visualise value_counts()\n",
    "    print(training_df[categoric_column+'_B'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_binning(training_df, validation_df, test_df, 'proto', 0.925)\n",
    "categoric_binning(training_df, validation_df, test_df, 'state', 0.99)\n",
    "categoric_binning(training_df, validation_df, test_df, 'service', 0.99)\n",
    "# REMINDER YOU CAN ITERATE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_features(df):\n",
    "\n",
    "    df['login_ports']=df['is_ftp_login'].astype(str)+df['is_sm_ips_ports'].astype(str)\n",
    "    df['login_flow']=df['is_ftp_login'].astype(str)+df['ct_ftp_cmd'].astype(str)\n",
    "    df['login_ports_flow']=df['is_ftp_login'].astype(str)+df['ct_ftp_cmd'].astype(str)+df['is_sm_ips_ports'].astype(str)\n",
    "\n",
    "concat_features(training_df)\n",
    "concat_features(validation_df)\n",
    "concat_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for sttl\n",
    "bins = [0, 25, 50, 100, 200, 1000]\n",
    "labels = [1, 32, 64, 128, 256]\n",
    "\n",
    "training_df['sttl_B'] = pd.cut(training_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)\n",
    "validation_df['sttl_B'] = pd.cut(validation_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)\n",
    "test_df['sttl_B'] = pd.cut(test_df['sttl'], bins, labels = labels, include_lowest = True).astype(int)\n",
    "\n",
    "# REMINDER TO DROP NOT _B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trans Depth Feature\n",
    "training_df['trans_depth_B']=np.where(training_df['trans_depth']>0,1,0)\n",
    "validation_df['trans_depth_B']=np.where(validation_df['trans_depth']>0,1,0)\n",
    "test_df['trans_depth_B']=np.where(test_df['trans_depth']>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total packages features\n",
    "training_df['total_packets_FE']=training_df['dur']*training_df['rate']\n",
    "validation_df['total_packets_FE']=validation_df['dur']*validation_df['rate']\n",
    "test_df['total_packets_FE']=test_df['dur']*test_df['rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoder(training_df, validation_df, test_df, col, response):\n",
    "\n",
    "    \"\"\" function to mean encode categorical features \n",
    "        any missing values are imputed with mode \"\"\"\n",
    "\n",
    "    # Create dictionary\n",
    "    mean_encoding=training_df.groupby(col)[response].mean().to_dict()\n",
    "\n",
    "    # Apply to train\n",
    "    training_df[col+'_ME']=training_df[col].replace(mean_encoding)\n",
    "    training_df[col+'_ME']=training_df[col+'_ME'].fillna(training_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to valid\n",
    "    validation_df[col+'_ME']=validation_df[col].replace(mean_encoding)\n",
    "    validation_df[col+'_ME']=validation_df[col+'_ME'].fillna(validation_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to test\n",
    "    test_df[col+'_ME']=test_df[col].replace(mean_encoding)\n",
    "    test_df[col+'_ME']=test_df[col+'_ME'].fillna(test_df[col+'_ME'].mode()[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding and Mean encoding\n",
    "num_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "fe_categorical_columns = list(training_df.select_dtypes(exclude=num_dtypes).columns)\n",
    "unwanted = ['proto', 'state', 'service']\n",
    "fe_categorical_columns = [ele for ele in fe_categorical_columns if ele not in unwanted]\n",
    "\n",
    "training_df=training_df.merge(response_training_df, on='id', how='left')\n",
    "\n",
    "for i in fe_categorical_columns:\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    mean_encoder(training_df, validation_df, test_df, i, 'label')\n",
    "    \n",
    "    # Get one hot encoding of columns B\n",
    "    one_hot_train = pd.get_dummies(training_df[i], prefix=i)\n",
    "    one_hot_valid = pd.get_dummies(validation_df[i], prefix=i)\n",
    "    one_hot_test = pd.get_dummies(test_df[i], prefix=i)\n",
    "    \n",
    "    # Drop column B as it is now encoded\n",
    "    training_df = training_df.drop(i,axis = 1)\n",
    "    validation_df = validation_df.drop(i,axis = 1)\n",
    "    test_df = test_df.drop(i,axis = 1)\n",
    "    \n",
    "    # Join the encoded df\n",
    "    training_df = training_df.join(one_hot_train)\n",
    "    validation_df = validation_df.join(one_hot_valid)\n",
    "    test_df = test_df.join(one_hot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "def outlier_capping(training_df, validation_df, test_df, variable, multiplier):\n",
    "\n",
    "    ''' cap and collar the response variable '''\n",
    "\n",
    "    q1 = np.percentile(training_df[variable],25)\n",
    "    q3 = np.percentile(training_df[variable],75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (iqr * multiplier)\n",
    "    upper = q3 + (iqr * multiplier)\n",
    "\n",
    "    # adjust on train\n",
    "    training_df[variable] = np.where(training_df[variable]<=lower, lower, training_df[variable])\n",
    "    training_df[variable] = np.where(training_df[variable]>=upper, upper, training_df[variable])\n",
    "\n",
    "    # adjust on train\n",
    "    validation_df[variable] = np.where(validation_df[variable]<=lower, lower, validation_df[variable])\n",
    "    validation_df[variable] = np.where(validation_df[variable]>=upper, upper, validation_df[variable])\n",
    "\n",
    "    # adjust on train\n",
    "    test_df[variable] = np.where(test_df[variable]<=lower, lower, test_df[variable])\n",
    "    test_df[variable] = np.where(test_df[variable]>=upper, upper, test_df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap all numeric values which arent integers\n",
    "s=(training_df.dtypes==float)\n",
    "\n",
    "for i in s[s].keys():\n",
    "    \n",
    "    print(i)\n",
    "    outlier_capping(training_df, validation_df, test_df, i, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all columns\n",
    "training_columns=list(training_df.columns)\n",
    "validation_columns=list(validation_df.columns)\n",
    "test_columns=list(test_df.columns)\n",
    "\n",
    "# intersection of all common items in train, valid, test\n",
    "modelling_cols=list(set(training_columns).intersection(validation_columns, test_columns))\n",
    "\n",
    "unwanted = ['proto', 'state', 'service', 'id']\n",
    "modelling_cols = [ele for ele in modelling_cols if ele not in unwanted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static functions\n",
    "\n",
    "def score(params):\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model.fit(X_train, \n",
    "              Y_train, \n",
    "              eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "              verbose=False, \n",
    "              early_stopping_rounds=25)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    Y_pred_test = model.predict(X_valid)\n",
    "    train_score = (-1*f1_score(Y_train, Y_pred_train))\n",
    "    test_score = (-1*f1_score(Y_valid, Y_pred_test))\n",
    "#     print('Training loss:', round(abs(train_score),4), 'Test loss:', round(abs(test_score),4))\n",
    "    return {'loss': test_score, 'status': STATUS_OK}  \n",
    "\n",
    "def optimize(trials, space, evals):\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=evals) # up this amount\n",
    "    return best\n",
    "\n",
    "def model_performance_metrics(y_train, train_pred, train_prob):\n",
    "    \n",
    "    ''' function to return the model performance metrics '''\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(train_pred, y_train).ravel()\n",
    "    print('True Negatives:', tn, 'False Positives:', fp, 'False Negatives:', fn, 'True Positives:', tp)\n",
    "    print('Accuracy Score:', accuracy_score(y_train, train_pred))\n",
    "    print('Precision Score:', precision_score(y_train, train_pred, average=None))\n",
    "    print('Recall Score:', recall_score(y_train, train_pred, average=None))\n",
    "    print('F1 Score:', f1_score(y_train, train_pred, average=None))\n",
    "    print('AUROC:', roc_auc_score(y_train, train_prob))\n",
    "    print('MCC:', matthews_corrcoef(y_train, train_pred), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, training_df, validation_df, test_df, X_train, Y_train, X_valid, Y_valid, metric, first_pass_rounds, second_pass_rounds):\n",
    "\n",
    "    ''' function to create XGBoost model, determine the most important features then re-run with reduced feature set '''\n",
    "    \n",
    "    # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "    # ratio = float(np.sum(response_training_df['label'] == 0)) / np.sum(response_training_df['label']==1)\n",
    "    # scale_pos_weight_val=round(ratio,0)\n",
    "    \n",
    "    # Choose hyperparameter search space\n",
    "    space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(2, 25, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(50, 12000, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.4, 0.9, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(1, 12, 1, dtype=int)),\n",
    "        #'scale_pos_weight':hp.choice('scale_pos_weight', np.arange(1, scale_pos_weight_val, 1, dtype=int)),   \n",
    "        'lambda':hp.choice('lambda', np.arange(1, 5, 1, dtype=int)),    \n",
    "        'subsample':hp.quniform('subsample', 0.6, 1.0, 0.1),\n",
    "        'gamma':hp.quniform('gamma', 0, 10, 1),\n",
    "        'objective':'binary:logistic',\n",
    "        'eta':hp.quniform('eta', 0.01, 0.2, 0.1),\n",
    "        'eval_metric': metric,\n",
    "    }\n",
    "        \n",
    "    print('*'*20)\n",
    "    print('Run initial set of models to begin optimisation')\n",
    "    print('*'*20)\n",
    "    trials = Trials()\n",
    "    best_params = optimize(trials, space, first_pass_rounds)\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Return the best parameters from optimisation')\n",
    "    print('*'*20)\n",
    "    parameters=space_eval(space, best_params)\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Create first pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    xgb_model = XGBClassifier(colsample_bytree=parameters['colsample_bytree'],\n",
    "     eta=parameters['eta'],\n",
    "     eval_metric=parameters['eval_metric'],\n",
    "     gamma=parameters['gamma'],\n",
    "     max_depth=parameters['max_depth'],\n",
    "     min_child_weight=parameters['min_child_weight'],\n",
    "     objective=parameters['objective'],\n",
    "     subsample=parameters['subsample'])\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Fit first pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    xgb_model.fit(\n",
    "        X_train, \n",
    "        Y_train,\n",
    "        eval_metric=metric,\n",
    "        eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "        verbose=True, \n",
    "        early_stopping_rounds=25)\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Results from first pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    model_performance_metrics(Y_valid, xgb_model.predict(X_valid), xgb_model.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Filter columns based on importance')\n",
    "    print('*'*20)\n",
    "    results=pd.DataFrame()\n",
    "    results['columns']=(list(training_df[modelling_cols]))\n",
    "    results['importances']=(list(xgb_model.feature_importances_))\n",
    "    refined_modelling_cols=list(results.loc[results['importances']>0.005]['columns'])\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Run second pass using reduced feature set')\n",
    "    print('*'*20)\n",
    "    X_train=training_df[refined_modelling_cols]\n",
    "    X_valid=validation_df[refined_modelling_cols]\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Run optimisation of second set of models')\n",
    "    print('*'*20)\n",
    "    trials = Trials()\n",
    "    best_params = optimize(trials, space, second_pass_rounds) \n",
    "\n",
    "    print('*'*20)\n",
    "    print('Return the best parameters from the second optimisation')\n",
    "    print('*'*20)\n",
    "    parameters=space_eval(space, best_params)\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Create second pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    xgb_model = XGBClassifier(colsample_bytree=parameters['colsample_bytree'],\n",
    "     eta=parameters['eta'],\n",
    "     eval_metric=parameters['eval_metric'],\n",
    "     gamma=parameters['gamma'],\n",
    "     max_depth=parameters['max_depth'],\n",
    "     min_child_weight=parameters['min_child_weight'],\n",
    "     n_estimators=parameters['n_estimators'],\n",
    "     objective=parameters['objective'],\n",
    "     subsample=parameters['subsample'])\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Fit second pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    xgb_model.fit(\n",
    "        X_train, \n",
    "        Y_train,\n",
    "        eval_metric=metric, \n",
    "        eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "        verbose=True, \n",
    "        early_stopping_rounds=25)\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Results from second pass XGBoost model')\n",
    "    print('*'*20)\n",
    "    model_performance_metrics(Y_valid, xgb_model.predict(X_valid), xgb_model.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Score on training, validation and test df')\n",
    "    print('*'*20)\n",
    "    training_df[model] = xgb_model.predict_proba(X_train)[:, 1]\n",
    "    validation_df[model] = xgb_model.predict_proba(X_valid)[:, 1]\n",
    "    test_df[model] = xgb_model.predict_proba(test_df[refined_modelling_cols])[:, 1]\n",
    "    \n",
    "    print('*'*20)\n",
    "    print('Print feature importance')\n",
    "    print('*'*20)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,15)\n",
    "    feature_important = xgb_model.get_booster().get_score(importance_type='weight')\n",
    "    keys = list(feature_important.keys())\n",
    "    values = list(feature_important.values())\n",
    "    data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=True)\n",
    "    data.plot(kind='barh')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list for prediction cols\n",
    "prediction_cols=[]\n",
    "\n",
    "# iterate through all levels of attack cat to produce models\n",
    "for i in list(set(response_training_df['attack_cat'])):\n",
    "    \n",
    "    print('*'*40)\n",
    "    print('*'*40)\n",
    "    print(f'Creating model predictions for {i}')\n",
    "    print('*'*40)\n",
    "    print('*'*40)\n",
    "    response_training_df[i]=np.where(response_training_df['attack_cat']==i,1,0)\n",
    "    response_validation_df[i]=np.where(response_validation_df['attack_cat']==i,1,0)\n",
    "    \n",
    "    # define features and target\n",
    "    X_train=training_df[modelling_cols]\n",
    "    Y_train=response_training_df[i]\n",
    "    X_valid=validation_df[modelling_cols]\n",
    "    Y_valid=response_validation_df[i]\n",
    "\n",
    "    # run model\n",
    "    classifier(i+'_error', training_df, validation_df, test_df, X_train, Y_train, X_valid, Y_valid, 'error', first_pass_rounds=2, second_pass_rounds=2)\n",
    "#     classifier(i+'_auc', training_df, validation_df, test_df, X_train, Y_train, X_valid, Y_valid, 'auc', first_pass_rounds=10, second_pass_rounds=50)\n",
    "    \n",
    "    # ensemble\n",
    "    training_df[i+'_prediction']=training_df[i+'_error']\n",
    "    validation_df[i+'_prediction']=validation_df[i+'_error']\n",
    "    test_df[i+'_prediction']=test_df[i+'_error']\n",
    "#     training_df[i+'_prediction']=(training_df[i+'_error']+training_df[i+'_auc'])/2\n",
    "#     validation_df[i+'_prediction']=(validation_df[i+'_error']+validation_df[i+'_auc'])/2\n",
    "#     test_df[i+'_prediction']=(test_df[i+'_error']+test_df[i+'_auc'])/2\n",
    "    \n",
    "    # add output to prediction_cols\n",
    "    prediction_cols.append(i+'_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_cutoff(act, pred):\n",
    "    \n",
    "    \"\"\" find the optimal probability cutoff point for a classification model \n",
    "        credit to https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(act, pred)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'value' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['value']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['classification_prediction']=training_df['Normal_prediction']\n",
    "validation_df['classification_prediction']=validation_df['Normal_prediction']\n",
    "test_df['classification_prediction']=test_df['Normal_prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal cut off\n",
    "val=optimal_cutoff(Y_train, training_df['classification_prediction'])\n",
    "print(val)\n",
    "\n",
    "# apply cut off\n",
    "validation_df['label_prediction']=np.where(validation_df['classification_prediction']<=val[0],0,1) # TO DO OPTIMISE HERE\n",
    "test_df['label_prediction']=np.where(test_df['classification_prediction']<=val[0],0,1)\n",
    "\n",
    "# review performance on validation\n",
    "model_performance_metrics(Y_valid, validation_df['label_prediction'], validation_df['classification_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df, col):\n",
    "    df[col+'_z']=(df[col]-df[col].mean())/df[col].std()\n",
    "\n",
    "def f(df):\n",
    "    if df['Analysis_prediction_z']==df['max']:\n",
    "        return 'Analysis'\n",
    "    elif df['Backdoor_prediction_z']==df['max']:\n",
    "        return 'Backdoor'\n",
    "    elif df['Exploits_prediction_z']==df['max']:\n",
    "        return 'Exploits'\n",
    "    elif df['Generic_prediction_z']==df['max']:\n",
    "        return 'Generic'\n",
    "    elif df['Reconnaissance_prediction_z']==df['max']:\n",
    "        return 'Reconnaissance'\n",
    "    elif df['Shellcode_prediction_z']==df['max']:\n",
    "        return 'Shellcode'\n",
    "    elif df['Fuzzers_prediction_z']==df['max']:\n",
    "        return 'Fuzzers'\n",
    "    elif df['Worms_prediction_z']==df['max']:\n",
    "        return 'Worms'\n",
    "    elif df['DoS_prediction_z']==df['max']:\n",
    "        return 'DoS'\n",
    "    else:\n",
    "        return 'Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zprediction_cols=[s + '_z' for s in prediction_cols]\n",
    "   \n",
    "for i in prediction_cols:\n",
    "    \n",
    "    # create z scores\n",
    "    z_score(training_df, i)\n",
    "    z_score(validation_df, i)\n",
    "    z_score(test_df, i)\n",
    "    \n",
    "# create max of cols\n",
    "training_df['max']=training_df[zprediction_cols].max(axis=1)\n",
    "validation_df['max']=validation_df[zprediction_cols].max(axis=1)\n",
    "test_df['max']=test_df[zprediction_cols].max(axis=1)\n",
    "\n",
    "# apply conversion\n",
    "training_df['m_classification_prediction'] = training_df.apply(f, axis=1)\n",
    "validation_df['m_classification_prediction'] = validation_df.apply(f, axis=1)\n",
    "test_df['m_classification_prediction'] = test_df.apply(f, axis=1)    \n",
    "\n",
    "# convert to export format\n",
    "training_df['attack_cat_prediction']=training_df['m_classification_prediction'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "                                     [0,1,2,3,4,5,6,7,8,9])\n",
    "validation_df['attack_cat_prediction']=validation_df['m_classification_prediction'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "                                     [0,1,2,3,4,5,6,7,8,9])\n",
    "test_df['attack_cat_prediction']=test_df['m_classification_prediction'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "                                     [0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform based on submission criteria\n",
    "# response_training_df['attack_cat']=response_training_df['attack_cat'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "#                                      [0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# response_validation_df['attack_cat']=response_validation_df['attack_cat'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "#                                      [0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "# X_train=training_df[modelling_cols]\n",
    "# Y_train=response_training_df['attack_cat']\n",
    "# X_valid=validation_df[modelling_cols]\n",
    "# Y_valid=response_validation_df['attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# # Choose hyperparameter search space\n",
    "# space = {\n",
    "#         'max_depth':hp.choice('max_depth', np.arange(2, 25, 1, dtype=int)),\n",
    "# #         'n_estimators':hp.choice('n_estimators', np.arange(50, 12000, 10, dtype=int)),\n",
    "#         'colsample_bytree':hp.quniform('colsample_bytree', 0.4, 0.9, 0.1),\n",
    "#         'min_child_weight':hp.choice('min_child_weight', np.arange(1, 12, 1, dtype=int)),\n",
    "# #         'scale_pos_weight':hp.choice('scale_pos_weight', np.arange(1, scale_pos_weight_val, 1, dtype=int)),   \n",
    "#         'lambda':hp.choice('lambda', np.arange(1, 5, 1, dtype=int)),    \n",
    "#         'subsample':hp.quniform('subsample', 0.6, 1.0, 0.1),\n",
    "#         'gamma':hp.quniform('gamm', 0, 10, 1),\n",
    "#         'objective':'multi:softmax',\n",
    "#         'eta':hp.quniform('eta', 0.1, 0.5, 0.1),\n",
    "#         'num_class':10,\n",
    "#         'eval_metric': 'merror',\n",
    "#     }\n",
    "\n",
    "# def score(params):\n",
    "#     model = XGBClassifier(**params)\n",
    "    \n",
    "#     model.fit(X_train, \n",
    "#               Y_train, \n",
    "#               eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "#               verbose=False, \n",
    "#               early_stopping_rounds=25)\n",
    "#     Y_pred_train = model.predict(X_train)\n",
    "#     Y_pred_test = model.predict(X_valid)\n",
    "    \n",
    "#     # F1 Score\n",
    "#     train_score = (f1_score(Y_train, Y_pred_train, average='macro'))\n",
    "#     test_score = (f1_score(Y_valid, Y_pred_test, average='macro'))\n",
    "    \n",
    "#     print('F1 Training loss:', round(abs(train_score),4), 'F1 Test loss:', round(abs(test_score),4))\n",
    "#     return {'loss': test_score, 'status': STATUS_OK}  \n",
    "\n",
    "# def optimize(trials, space):\n",
    "    \n",
    "#     best = fmin(score, space, algo=tpe.suggest, max_evals=10) # up this amount\n",
    "#     return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimise\n",
    "# trials = Trials()\n",
    "# best_params = optimize(trials, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Return the best parameters\n",
    "# parameters=space_eval(space, best_params)\n",
    "# print(parameters)\n",
    "\n",
    "# # Apply to model\n",
    "# xgb_model_mc = XGBClassifier(colsample_bytree=parameters['colsample_bytree'],\n",
    "#  eta=parameters['eta'],\n",
    "#  eval_metric=parameters['eval_metric'],\n",
    "#  gamma=parameters['gamma'],\n",
    "#  max_depth=parameters['max_depth'],\n",
    "#  min_child_weight=parameters['min_child_weight'],\n",
    "# #  n_estimators=parameters['n_estimators'],\n",
    "# #  scale_pos_weight=parameters['scale_pos_weight'],\n",
    "#  objective=parameters['objective'],\n",
    "#  num_class=parameters['num_class'],\n",
    "#  subsample=parameters['subsample'])\n",
    "\n",
    "# xgb_model_mc.fit(\n",
    "#     X_train, \n",
    "#     Y_train,\n",
    "#     eval_metric=\"merror\", \n",
    "#     eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "#     verbose=True, \n",
    "#     early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Make predictions on training set\n",
    "# predictions = xgb_model_mc.predict(X_valid)\n",
    "\n",
    "# #Print accuracy\n",
    "# accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "# print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "# f1=f1_score(Y_valid, predictions, average=None)\n",
    "# print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score on training_df and validation_df\n",
    "# training_df['predictions_mc'] = xgb_model_mc.predict(X_train)\n",
    "# validation_df['predictions_mc'] = xgb_model_mc.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Inputs\n",
    "y_prediction=validation_df['label_prediction']\n",
    "y_true_response=response_validation_df['label']\n",
    "\n",
    "f1_score(y_true_response, y_prediction) # average='macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction=validation_df['attack_cat_prediction']\n",
    "\n",
    "response_validation_df['attack_cat_conv']=response_validation_df['attack_cat'].replace(['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode','Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],\n",
    "                                     [0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "y_true_response=response_validation_df['attack_cat_conv']\n",
    "\n",
    "f1_score(y_true_response, y_prediction, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score xgb_model on test\n",
    "y_test_id = test_df['id']\n",
    "y_test_label_pred = test_df['label_prediction']\n",
    "y_test_attack_cat_pred = test_df['attack_cat_prediction']\n",
    "\n",
    "# export for submission\n",
    "y_pred = pd.DataFrame(data = {'id': y_test_id, 'label' : y_test_label_pred, 'attack_cat' : y_test_attack_cat_pred})\n",
    "y_pred.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
